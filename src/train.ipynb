{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb423019",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "from layers.embedding import EmbeddingLayer\n",
    "from layers.rnn.bidirectionalRNN import BidirectionalRNN\n",
    "from layers.rnn.unidirectionalRNN import UnidirectionalRNN\n",
    "from layers.dense import DenseLayer\n",
    "\n",
    "from something.model import Model\n",
    "from something.rnn import RNN\n",
    "from utils.evaluate import evaluate_model\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e13c0d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')\n",
    "valid = pd.read_csv('data/valid.csv')\n",
    "train['label'] = train['label'].map({'neutral': 0, 'positive': 1, 'negative': 2}).astype(np.float32)\n",
    "test['label'] = test['label'].map({'neutral': 0, 'positive': 1, 'negative': 2}).astype(np.float32)\n",
    "valid['label'] = valid['label'].map({'neutral': 0, 'positive': 1, 'negative': 2}).astype(np.float32)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "seed = 42\n",
    "random.seed(seed)                         # python random\n",
    "np.random.seed(seed)                      # numpy\n",
    "tf.random.set_seed(seed)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bc80393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 2796\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(oov_token=\"<UNK>\")  # Reserve a token for unknown words\n",
    "tokenizer.fit_on_texts(train['text'].values) \n",
    "vocab_size = len(tokenizer.word_index) + 1  # +1 for padding (index 0 is reserved)\n",
    "print(f\"Vocabulary size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2834b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100 # Dimension of the embedding layer\n",
    "max_length = 100  # Maximum length of input sequences\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(train['text'].values)\n",
    "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "valid_sequences = tokenizer.texts_to_sequences(valid['text'].values)\n",
    "valid_padded = pad_sequences(valid_sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "test_sequences = tokenizer.texts_to_sequences(test['text'].values)\n",
    "test_padded = pad_sequences(test_sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1  # +1 for padding (index 0 is reserved)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b25658b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train padded shape: (500, 100)\n",
      "Valid padded shape: (100, 100)\n",
      "Test padded shape: (400, 100)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train padded shape: {train_padded.shape}\")\n",
    "print(f\"Valid padded shape: {valid_padded.shape}\")\n",
    "print(f\"Test padded shape: {test_padded.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5935407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "Macro F1 Score: 0.2503\n",
      "Embedding weights shape: (2796, 100)\n",
      "RNN weights shape: (100, 16), (16, 16), (16,)\n",
      "Dense weights shape: (16, 3), (3,)\n",
      "Macro F1 Score: 0.2503\n"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_dim),\n",
    "    SimpleRNN(units=16, activation='tanh', return_sequences=False),\n",
    "    # Dropout(0.5),\n",
    "    Dense(3, activation='softmax')  # for binary classification (e.g., sentiment)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(\n",
    "    train_padded,\n",
    "    train['label'].values,\n",
    "    validation_data=(valid_padded, valid['label'].values),\n",
    "    epochs=10,          # number of passes over the data, adjust as needed\n",
    "    batch_size=32,      # number of samples per batch, adjust as needed\n",
    "    verbose=0          # verbosity mode, 0 = silent, 1 = progress bar, 2 = one line per epoch\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(model, test_padded, test['label'])\n",
    "\n",
    "embedding_layer = model.layers[0]\n",
    "embedding_weights = embedding_layer.get_weights()[0]\n",
    "rnn_layer = model.layers[1]\n",
    "rnn_weights = rnn_layer.get_weights()\n",
    "dense_layer = model.layers[2]\n",
    "dense_weights = dense_layer.get_weights()\n",
    "\n",
    "print(f\"Embedding weights shape: {embedding_weights.shape}\")\n",
    "print(f\"RNN weights shape: {rnn_weights[0].shape}, {rnn_weights[1].shape}, {rnn_weights[2].shape}\")\n",
    "print(f\"Dense weights shape: {dense_weights[0].shape}, {dense_weights[1].shape}\")\n",
    "\n",
    "embedding_layer_scratch = EmbeddingLayer(vocab_size, embedding_dim).load_weights(embedding_weights)\n",
    "rnn_layer_scratch = UnidirectionalRNN(16, 100, 32, \"tanh\").load_weights(rnn_weights)\n",
    "dense_layer_scratch = DenseLayer(3, 16, activation='softmax', init_method='zeros').load_weights(dense_weights)\n",
    "\n",
    "modelScratch = RNN([\n",
    "    embedding_layer_scratch,\n",
    "    rnn_layer_scratch,\n",
    "    dense_layer_scratch\n",
    "], 32)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(modelScratch, test_padded, test['label'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
